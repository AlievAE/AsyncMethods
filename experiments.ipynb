{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'methods'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tl/xqt8rvfj2_x_0216w6nc9xyh0000gn/T/ipykernel_28260/3360432737.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmethods\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinibatchSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsynchronousGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRennalaSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWorkerState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mLinearRegression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_regression_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_regression_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mLogisticRegression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogistic_regression_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogistic_regression_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mMLP_mnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_from_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_from_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'methods'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/eneminova/AsyncMethods')\n",
    "\n",
    "from methods import MinibatchSGD, AsynchronousGD, RennalaSGD, WorkerState\n",
    "from LinearRegression import linear_regression_loss, linear_regression_gradient\n",
    "from LogisticRegression import logistic_regression_loss, logistic_regression_gradient\n",
    "from MLP_mnist import SimpleNN, loss_from_vector, gradient_from_vector\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from functools import partial\n",
    "import scipy.stats as sps\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy.stats as sps\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import ortho_group\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LinReg on diabetes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X = np.hstack((diabetes_X, np.ones((len(diabetes_X), 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 4\n",
    "time_distributions = [sps.norm(i+1, scale=1) for i in range(n_workers)]\n",
    "batch_sizes = np.arange(1, n_workers + 1) * 10\n",
    "\n",
    "\n",
    "n_iterations = 10000\n",
    "\n",
    "w = np.zeros(diabetes_X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_sgd = MinibatchSGD(w, (diabetes_X, diabetes_y), time_distributions, linear_regression_loss, \\\n",
    "                              [partial(linear_regression_gradient, batch_size=batch_sizes[i]) for i in range(n_workers)], \\\n",
    "                                  learning_rate=lr)\n",
    "async_gd = AsynchronousGD(w, (diabetes_X, diabetes_y), time_distributions, linear_regression_loss, \\\n",
    "                              [partial(linear_regression_gradient, batch_size=batch_sizes[i]) for i in range(n_workers)], \\\n",
    "                                  learning_rate=lr)\n",
    "rennala_sgd = RennalaSGD(w, (diabetes_X, diabetes_y), time_distributions, linear_regression_loss, \\\n",
    "                              [partial(linear_regression_gradient, batch_size=batch_sizes[i]) for i in range(n_workers)], \\\n",
    "                                  learning_rate=lr)\n",
    "rennala_sgd.set_batch_size(5)\n",
    "\n",
    "methods = {'Minibatch SGD' : minibatch_sgd,\n",
    "           'Async GD' : async_gd,\n",
    "           'Rennala SGD' : rennala_sgd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = dict()\n",
    "losses = dict()\n",
    "for method_name in methods:\n",
    "    w = np.zeros(diabetes_X.shape[1])\n",
    "    current_x, loss_history, computation_times, x_history = methods[method_name].run_steps(n_iterations)\n",
    "    times[method_name] = computation_times\n",
    "    losses[method_name] = loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for method_name in methods:\n",
    "    plt.plot(times[method_name], losses[method_name], label=method_name)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Optimization Methods Comparison, Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogReg on Mushroom**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "path = kagglehub.dataset_download(\"prishasawhney/mushroom-dataset\")\n",
    "\n",
    "data = pd.read_csv(f\"{path}/mushroom_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "x = data.drop('class', axis=1).values\n",
    "x = scaler.fit_transform(x)\n",
    "y = 2*data['class'].values - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mushroom = (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "w = np.zeros(x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_sgd = MinibatchSGD(w, data_mushroom, time_distributions, logistic_regression_loss, \\\n",
    "                              [partial(logistic_regression_gradient, batch_size=batch_sizes[i]) for i in range(n_workers)], \\\n",
    "                                  learning_rate=lr)\n",
    "async_gd = AsynchronousGD(w, data_mushroom, time_distributions, logistic_regression_loss, \\\n",
    "                              [partial(logistic_regression_gradient, batch_size=batch_sizes[i]) for i in range(n_workers)], \\\n",
    "                                  learning_rate=lr)\n",
    "rennala_sgd = RennalaSGD(w, data_mushroom, time_distributions, logistic_regression_loss, \\\n",
    "                              [partial(logistic_regression_gradient, batch_size=batch_sizes[i]) for i in range(n_workers)], \\\n",
    "                                  learning_rate=lr)\n",
    "rennala_sgd.set_batch_size(5)\n",
    "\n",
    "methods = {'Minibatch SGD' : minibatch_sgd,\n",
    "           'Async GD' : async_gd,\n",
    "           'Rennala SGD' : rennala_sgd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = dict()\n",
    "losses = dict()\n",
    "for method_name in methods:\n",
    "    w = np.zeros(diabetes_X.shape[1])\n",
    "    current_x, loss_history, computation_times, x_history = methods[method_name].run_steps(1000)\n",
    "    times[method_name] = computation_times\n",
    "    losses[method_name] = loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for method_name in methods:\n",
    "    plt.plot(times[method_name], losses[method_name], label=method_name)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Optimization Methods Comparison, Logistic Regression on Mushroom')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(logistic_regression_gradient(data_mushroom, x_history[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NN on MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize to mean 0.5, std 0.5\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "subset_indices = torch.randperm(len(mnist_train))[:k]\n",
    "subset_train = torch.utils.data.Subset(mnist_train, subset_indices)\n",
    "train_loader = DataLoader(subset_train, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12730"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_param = sum(p.numel() for p in model.parameters())\n",
    "num_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = torch.rand((1, num_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_sgd = MinibatchSGD(start, mnist_train, time_distributions, partial(loss_from_vector, model=model, criterion=criterion), \\\n",
    "                              [partial(gradient_from_vector, model=model, criterion=criterion, batch_size=batch_sizes[i]) for i in range(n_workers)], \\\n",
    "                                  learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = dict()\n",
    "losses = dict()\n",
    "w = np.zeros(diabetes_X.shape[1])\n",
    "current_x, loss_history, computation_times, x_history = minibatch_sgd.run_steps(1)\n",
    "times[method_name] = computation_times\n",
    "losses[method_name] = loss_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
